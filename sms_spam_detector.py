# -*- coding: utf-8 -*-
"""SMS SPAM DETECTOR.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1tS3XHhq3lT8b9VQuS-CA9HilpWNyqnvE

**SMS SPAM DETECTOR**
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.metrics import accuracy_score
from sklearn.metrics import classification_report,confusion_matrix
from sklearn.naive_bayes import MultinomialNB

import chardet

# Detect the encoding of the file
with open('/content/spam.csv', 'rb') as f:
    result = chardet.detect(f.read())

# Print the detected encoding
print(result)

# Read the CSV file using the detected encoding 'Windows-1252'
df = pd.read_csv('/content/spam.csv', encoding='Windows-1252')
df.head()

df.v1

"""**Exploratory Data Analysis (EDA)**"""

#Find count and unique messages count of all the messages
df.describe()

print(df.columns)

#Extract SPAM messages
spam_messages = df[df["v1"]=="spam"]
spam_messages.head() #Display first 5 rows of SPAM messages

#Find count and unique messages count of SPAM messages.
spam_messages.describe()

#Plot the counts of HAM (non SPAM) vs SPAM
sns.countplot(data = df, x= df["v1"]).set_title("Amount of spam and no-spam messages")
plt.show()

"""**Splitting the SMS data into Test and Train data**"""

# Rename columns
df.rename(columns={'v1': 'label', 'v2': 'text'}, inplace=True)

# Drop unnamed columns
df = df[['label', 'text']]

# Check the updated DataFrame
print(df.head())

data_train, data_test, labels_train, labels_test = train_test_split(df.text,df.label,test_size=0.2,random_state=0)

print("data_train, labels_train : ",data_train.shape, labels_train.shape)
print("data_test, labels_test: ",data_test.shape, labels_test.shape)

"""**Extraction & CountVectorize**

The CountVectorizer provides a simple way to both tokenize a collection of text documents and build a vocabulary of known words, but also to encode new documents using that vocabulary.
"""

vectorizer = CountVectorizer()
#fit & transform
# fit: build dict (i.e. word->wordID)
# transform: convert document (i.e. each line in the file) to word vector
data_train_count = vectorizer.fit_transform(data_train)
data_test_count  = vectorizer.transform(data_test)

"""**Modelling & training**"""

clf = MultinomialNB()
clf.fit(data_train_count, labels_train)
predictions = clf.predict(data_test_count)
predictions

"""**Results and Accuracy**"""

print ("accuracy_score : ", accuracy_score(labels_test, predictions))

print ("confusion_matrix : \n", confusion_matrix(labels_test, predictions))

print (classification_report(labels_test, predictions))